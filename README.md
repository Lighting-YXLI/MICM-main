The official code of our paper: 
# *Unlocking implicit motion for evaluating image complexity*


# MICM Pipeline
Pipeline of our proposed MICM. The model consists of a dual-branch: image branch and motion branch. 
the image branch takes images as input, and utilizes pre-trained CNN for feature extraction.
the motion branch takes videos and corresponding text prompt as inputs, the motion features are first generated by $F^{'}_{V}$ (which is denoted in spatial.py) and then calculate the
video-to-text alignment for score prediction.

# ICA Dataset
We conduct experiments on three public ICA databases: IC9600[https://github.com/tinglyfeng/IC9600], VISC-C[https://data.mendeley.com/datasets/7943zgtsr7/1], and Savoias[https://github.com/esaraee/Savoias-Dataset.].

# Training and Evaluation
Uncompress the dataset and put it in the working directory:
take the IC9600 as example:
> IC9600\
> ├── images\
> ├── test.txt\
> └── train.txt

The above `train.txt` and `test.txt` specify the training and testing images in our experiment, where the first column, second column, and third column in each row are image name, corresponding text prompt and ground truth complexity respectively.
The train.csv and test.csv files can be obtained by the trainvalsplit.py, and then please copy the data from csv file to corresponding txt file.
We display example files of train/test.txt in the released code.

Before training, the text prompt and video are pre-processed by the following steps:

1. the text prompt of images are generated following the protocols of InstructBLIP [https://github.com/salesforce/LAVIS/tree/main/projects/instructblip]

2. generate the corresponding videos following the protocols of I2V models such as CogVideoX [https://github.com/zai-org/CogVideo]. Then extract video 
features by ```python spatial.py```. We put the videos generated by CogVideoX for IC9600 dataset in the following link: [通过网盘分享的文件：ic9600videocogx5.zip
链接: https://pan.baidu.com/s/1z8ZlB5goGNtfiDDwixVRTQ?pwd=a5h9 提取码: a5h9 
--来自百度网盘超级会员v5的分享]

Finally, train the MICM:
```
python train.py 
```

Evaluation results will be printed on the terminal.

Note that the key environment used in our experiments are:
```
python==3.9.9
torch==1.8.1+cu111
torchvision==0.9.1+cu111
```
Note that the main backbone of MICM is detailed in the MICM.py

# Citation
If you find this repo useful, please cite the following publication:
```
@article{feng2023ic9600,
  title={IC9600: A Benchmark Dataset for Automatic Image Complexity Assessment},
  author={Feng, Tinglei and Zhai, Yingjie and Yang, Jufeng and Liang, Jie and Fan, Deng-Ping and Zhang, Jing and Shao, Ling and Tao, Dacheng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number={01},
  pages={1--17},
  year={2023},
  publisher={IEEE Computer Society},
  doi={10.1109/TPAMI.2022.3232328},
}
```
